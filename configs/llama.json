{
  "// 1": "Model Dimensions",
  "arch": "llama",
  "d_model": 3072,
  "n_layers": 28,
  "n_heads": 24,
  "n_kv_heads": 8,
  "d_ff": 8192,
  "vocab_size": 128256,
  "rope_base": 500000.0,
  "// 2": "Attention Mechanisms",
  "use_gqa": true,
  "use_mqa": false,
  "use_moe": false,
  "// 3": "Runtime & Memory",
  "batch_size": 1,
  "seq_len": 128,
  "block_size": 16,
  "weight_dtype": "bfloat16",
  "strict_load": false,
  "quantization": "W8A16",
  "// 4": "Weight Loading",
  "hf_repo_id": "mlx-community/Llama-3.2-3B-Instruct",
  "weight_file": "model-00001-of-00002.safetensors",
  "// 5": "Inference Settings",
  "max_new_tokens": 10,
  "// 6": "LoRA Fine-Tuning",
  "use_lora": true,
  "lora_rank": 8,
  "lora_alpha": 16.0,
  "lora_target_modules": [
    "q_proj",
    "v_proj"
  ],
  "lora_merge_after_training": false,
  "// 7": "Optimization, Speculative decoding (draft config in optmizations)",
  "speculative": true,
  "draft_layers": 8
}
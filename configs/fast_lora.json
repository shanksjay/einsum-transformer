{
    "// 1": "Model Dimensions (Scaled up for Profiling)",
    "arch": "llama",
    "d_model": 512,
    "n_layers": 8,
    "n_heads": 8,
    "n_kv_heads": 4,
    "d_ff": 2048,
    "vocab_size": 4096,
    "rope_base": 10000.0,
    "// 2": "Attention Mechanisms",
    "use_gqa": true,
    "use_mqa": false,
    "use_moe": false,
    "// 3": "Runtime & Memory",
    "batch_size": 4,
    "seq_len": 128,
    "block_size": 16,
    "weight_dtype": "float32",
    "strict_load": false,
    "quantization": "W8A16",
    "// 4": "Weight Loading (use configs/llama.json for Llama 3.2 3B; fast_lora arch does not match)",
    "hf_repo_id": null,
    "weight_file": null,
    "// 5": "Inference Settings",
    "max_new_tokens": 10,
    "// 6": "LoRA Fine-Tuning",
    "use_lora": false,
    "lora_rank": 8,
    "lora_alpha": 16.0,
    "lora_target_modules": [
        "q_proj",
        "v_proj"
    ],
    "lora_merge_after_training": false,
    "// 7": "Optimization, Speculative decoding (draft config in optmizations)",
    "speculative": true,
    "draft_layers": 8
}